# -*- coding: utf-8 -*-
"""py_spark.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gYUSrLQeKZFZks-g3asDB5WtlzZFFRI1
"""

!pip install pyspark

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql import functions as f
spark = SparkSession.builder.appName('job_transformation').getOrCreate()

df_pyspark = spark.read.csv("/content/beko.csv", sep=';', header='true', inferSchema=True)
df_pyspark.show()

df_pyspark.printSchema()

df_pyspark.head(1)

df_pyspark.select(f.col('article'), f.col('ozon_product_id1')).show()

df_pyspark.describe().show()

#Добавление новой колонки в DataFrame (test)
df_pyspark.withColumn('action_id_plus', df_pyspark['action_id']+1).show()

df_pyspark.drop('action_id').show()

df_pyspark.withColumnRenamed('action_id', 'action_id_new_2').show()

df_pyspark.drop('article').show()

#how='all' - удаляет записи все все null значения в строке; tresh - пороговое значения для количества null в строке (<=)
df_pyspark.na.drop(how='any', thresh=4).show()

#Можем удалять null в конкретном подмножестве
df_pyspark.na.drop(how='all', subset=['ozon_product_id1']).show()

df_pyspark.na.fill('Missing value').show()

df_pyspark.filter("price_current<=70000").show()

df_pyspark.filter("price_current<=70000").groupBy('article').count().show()